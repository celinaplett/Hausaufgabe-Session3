---
title: "Hausaufgabe Session 4"
output: html_document
--- 
#Laden der Libraries und Daten 
```{r}
library(tidyverse)
library(e1071)
library(caret)
library(pROC)
```

```{r}

titanic <- read_delim("titanic.csv", ";", 
    escape_double = FALSE, trim_ws = TRUE)
```
```{r}
titanic %>%
  group_by(survived) %>%
  summarize(n = n())
```
Aufgabe 1: 
Bitte erstellen Sie ein Notebook mit weiteren Features (Alter, Geschlecht und Klasse sind als Beispiel in meinem Notebook auf GitHub)

```{r}
(titanic.df <- titanic %>%
  select(survived,pclass,sex,age,embarked,parch,sibsp))
```

```{r}
titanic.df <- titanic.df %>%
  mutate(age = as.numeric(str_replace(age,",",".")))
```
```{r}
titanic.df <- titanic.df %>%
  mutate(parch = as.numeric(str_replace(parch,",",".")))
```
```{r}
titanic.df <- titanic.df %>%
  mutate(sibsp = as.numeric(str_replace(sibsp,",",".")))
```
```{r}
titanic.df <- na.omit(titanic.df)
```
```{r}
titanic.df <- titanic.df %>%
  mutate(sex = ifelse(sex == "female", 1, 0))
```
```{r}
titanic.df <- titanic.df %>%
  mutate(embarked = ifelse(embarked == "S", 1, 0))
```
```{r}
titanic.df <- titanic.df %>%
  mutate(sex = ifelse(sex == "female", 1, 0))
```

```{r}
set.seed(107)
inTrain <- createDataPartition(
  y = titanic.df$survived,
  p = .8,
  list = FALSE)
training <- titanic.df[ inTrain,]
testing  <- titanic.df[-inTrain,]
```

```{r}
model <- svm(survived ~ ., data = training)
summary(model)
pred <- predict(model, testing[,-1], probability = FALSE)
```
```{r}
(test.results <- cbind(pred, testing))
```

```{r}
test.results2 <- test.results %>%
  mutate(pred = ifelse(pred>=0.5,1,0))
table(test.results2$pred, testing$survived)
```

```{r}
pROC_obj <- roc(test.results$survived, test.results$pred,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
```
# Naive Bayes

```{r}
my_training <- training %>%
  mutate(survived = as.factor(survived))%>%
  mutate(sex = as.factor(sex))%>%
  mutate(pclass = as.factor(pclass)) %>%
  mutate(age = as.factor(age)) %>%
  mutate(embarked = as.factor(embarked)) %>%
  mutate(parch = as.factor(parch)) %>%
  mutate(sibsp = as.factor(sibsp))
model <- naiveBayes(survived ~ ., data = my_training)
model
```

```{r}
my_testing <- testing %>%
  mutate(sex = as.factor(sex)) %>%
  mutate(pclass = as.factor(pclass))%>%
  mutate(age = as.factor(age)) %>%
  mutate(embarked = as.factor(embarked)) %>%
  mutate(parch = as.factor(parch)) %>%
  mutate(sibsp = as.factor(sibsp))
pred <- predict(model, my_testing)
table(pred, my_testing$survived)
```


```{r}
(test.results <- cbind(pred, my_testing))
```

```{r}
test.results <- test.results %>%
  mutate(pred = as.numeric(pred))
pROC_obj <- roc(as.numeric(as.character(test.results$survived)), test.results$pred,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
```
# Decision Tree

```{r}
library(rpart)
library(rpart.plot)
tree<- rpart(survived~., data = training, method = 'class')
rpart.plot(tree)
```

```{r}
dt_results <- predict(tree, testing[,-1], type = 'prob')
head(model.results.dt <- cbind(testing,dt_results),500)
```

```{r}
test.results2 <- test.results %>%
  mutate(pred = ifelse(pred>=0.5,1,0))
table(test.results2$pred, testing$survived)
```


```{r}
pROC_obj <- roc(model.results.dt$survived,model.results.dt$`1`,
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
```
Aufgabe 2: 
Was sind die Unterschiede in der Performance der Algorithmen?

Die Performance des Naive Bayes ist am schlechtesten. Die Performance von dem Decision Tree sowie dem 1. Algorithmus liegen dagegen sehr nah beieinander. Generell lässt sich sagen, dass die Werte alle über 0,500 liegen und insgesamt nur um maximal 0,1 AUC auseinanderliegen. Zudem hat der erste Algorithmus eine sehr unregelmäßige Kurve, wohingegen die anderen beiden Graphen eine sehr regelmäßige Kurve aufzeigen. 

Aufgabe 3: 
Finden Sie Erklärungen dafür.

Die Unterschiede der Ergebnisse der Algorithmen liegen nur sehr gering auseinander. Dies könnte daran liegen, dass die Datensätze sehr sauber sind. Je größer der AUC-Wert ist, desto besser ist das Modell für den Datensatz geeignet. In diesem Fall sind also der 1. Algorithmus sowie der Decision Tree besser geeignet, als der Naive Bayes Alogrithmus. 